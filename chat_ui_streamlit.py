import sys
import ast
from pathlib import Path
import os
import streamlit as st
import tempfile
import os
import json
import subprocess

st.set_page_config(page_title="AIRA Chat", layout="wide")
st.title("AIRA: Recsys ë¦¬ì„œì¹˜/ì„¤ê³„ Assistant")

def call_aira(pdf_files, objective, kpi, data_desc, constraints, user_question=None):
    if not pdf_files:
        raise ValueError("ìµœì†Œ 1ê°œ ì´ìƒì˜ PDFë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")

    with tempfile.TemporaryDirectory() as tmpdir:
        pdf_paths = []
        for f in pdf_files:
            path = os.path.join(tmpdir, f.name)
            with open(path, "wb") as out:
                out.write(f.getbuffer())
            pdf_paths.append(path)

        repo_root = Path(__file__).resolve().parent
        run_agent_path = repo_root / "scripts" / "run_agent.py"

        cmd = [
            sys.executable,
            str(run_agent_path),
            "--pdf", *pdf_paths,
            "--objective", objective,
            "--kpi", kpi,
            "--data", data_desc,
            "--constraints", constraints,
        ]

        if user_question:
            objective_with_q = objective + f"\n\n[ì‚¬ìš©ì ì¶”ê°€ ì§ˆë¬¸]\n{user_question}"
            cmd[cmd.index("--objective") + 1] = objective_with_q

        # ìì‹ í”„ë¡œì„¸ìŠ¤ëŠ” UTF-8ë¡œ ì¶œë ¥í•˜ê²Œ
        env = os.environ.copy()
        env["PYTHONIOENCODING"] = "utf-8"

        # bytesë¡œ ë°›ê³  ë‚˜ì¤‘ì— ìš°ë¦¬ê°€ ë””ì½”ë”©
        result = subprocess.run(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            env=env,
        )

        raw_stdout = (result.stdout or b"").decode("utf-8", errors="ignore")
        raw_stderr = (result.stderr or b"").decode("utf-8", errors="ignore")

        if result.returncode != 0:
            # run_agent.py ë‚´ë¶€ ì—ëŸ¬ëŠ” stderr ê·¸ëŒ€ë¡œ ë³´ì—¬ì£¼ì
            raise RuntimeError(
                f"run_agent.py ì‹¤í–‰ ì‹¤íŒ¨ (code={result.returncode})\n\nSTDERR:\n{raw_stderr}"
            )

        text = raw_stdout.strip()
        if not text:
            raise RuntimeError("run_agent.py stdoutì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

        # stdout ì „ì²´ì—ì„œ { ... } ë¸”ë¡ë§Œ í•œ ë²ˆ ì‹œë„
        start = text.find("{")
        end = text.rfind("}")

        payload = None

        if start != -1 and end != -1 and end > start:
            json_like = text[start:end + 1]
            # 1ì°¨: JSON ì‹œë„
            try:
                payload = json.loads(json_like)
            except Exception:
                # JSON ì•„ë‹ˆë©´ ê·¸ëƒ¥ í¬ê¸°í•˜ê³  ì•„ë˜ì—ì„œ raw í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬
                payload = None

        # ê·¸ë˜ë„ payloadë¥¼ ëª» ë§Œë“¤ì—ˆìœ¼ë©´, ê·¸ëƒ¥ raw í…ìŠ¤íŠ¸ í†µì§¸ë¡œ ë°˜í™˜
        if payload is None:
            payload = {"raw": text}

        return payload, raw_stdout.splitlines()

st.sidebar.header("ì‹¤í—˜ ì„¤ì •")

uploaded_pdfs = st.sidebar.file_uploader(
    "ë…¼ë¬¸ PDF ì—…ë¡œë“œ (ì—¬ëŸ¬ ê°œ ê°€ëŠ¥)",
    type=["pdf"],
    accept_multiple_files=True,
)

default_objective = "ê³ ê°€ì¹˜ ì‚¬ìš©ì ìœ ì§€ìœ¨ ê°œì„ ì„ ìœ„í•œ next-item ì¶”ì²œ"
default_kpi = "NDCG@10 + latency < 80ms"
default_data = "GA4 ì´ë²¤íŠ¸, ìƒí’ˆ ë§ˆìŠ¤í„°, 1.8ì–µ íŠ¸ëœì­ì…˜"
default_constraints = "Vertex AI ì‹¤ì‹œê°„ ì„œë¹™, ë¹„ìš© ì›” 5ì²œë¶ˆ í•œë„"

objective = st.sidebar.text_area("Objective", value=default_objective)
kpi = st.sidebar.text_input("KPI", value=default_kpi)
data_desc = st.sidebar.text_area("Data description", value=default_data)
constraints = st.sidebar.text_area("Constraints", value=default_constraints)

show_raw = st.sidebar.checkbox("raw stdout ë³´ê¸°", value=False)

# ---------- ì„¸ì…˜ ìƒíƒœ ----------
if "messages" not in st.session_state:
    st.session_state.messages = []

# ê¸°ì¡´ ë©”ì‹œì§€ ë Œë”ë§
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# ---------- ì…ë ¥ ----------
user_input = st.chat_input("AIRAì—ê²Œ ì§ˆë¬¸í•´ ë³´ì„¸ìš” (Multi-Turn ì ìš© ì˜ˆì •).")

if user_input:
    # 1) ìœ ì € ë©”ì‹œì§€ ì¶”ê°€
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    # 2) AIRA í˜¸ì¶œ
    try:
        with st.chat_message("assistant"):
            with st.spinner("AIRA ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘..."):
                payload, raw_logs = call_aira(
                    uploaded_pdfs,
                    objective=objective,
                    kpi=kpi,
                    data_desc=data_desc,
                    constraints=constraints,
                    user_question=user_input,
                )

            # --- ì‘ë‹µ êµ¬ì¡° íŒŒì‹± ---
            config = payload.get("config", {})
            meta = payload.get("paper_taxonomy", {}).get("meta", {})
            roadmap = payload.get("roadmap", {})

            # outputs ìš°ì„ , ì—†ìœ¼ë©´ roadmap.legacy_outputs, ê·¸ë˜ë„ ì—†ìœ¼ë©´ result/payload
            data = (
                payload.get("outputs")
                or roadmap.get("legacy_outputs")
                or payload.get("result")
                or payload
            )

            sections = []

            # 0. ì„¤ì • / ë©”íƒ€ ì •ë³´ ê°„ë‹¨ ìš”ì•½
            header_lines = []
            if config.get("objective"):
                header_lines.append(f"- **Objective**: {config['objective']}")
            if config.get("kpi"):
                header_lines.append(f"- **KPI**: {config['kpi']}")
            if meta.get("title"):
                header_lines.append(f"- **ë…¼ë¬¸ ì œëª©**: {meta['title']}")
            if meta.get("year"):
                header_lines.append(f"- **ì—°ë„**: {meta['year']}")
            if header_lines:
                sections.append("#### âš™ï¸ ì„¤ì • / ë©”íƒ€\n" + "\n".join(header_lines))

            # 1. ë…¼ë¬¸ ìš”ì•½ / ìš”êµ¬ì‚¬í•­ / ì•„í‚¤í…ì²˜ / ì‹¤í—˜ í”Œëœ
            if isinstance(data, dict):
                if "research_summary" in data:
                    sections.append("### ğŸ” ë…¼ë¬¸ ìš”ì•½\n" + data["research_summary"])
                if "requirements_analysis" in data:
                    sections.append("### ğŸ¯ ìš”êµ¬ì‚¬í•­ ì •ë ¬\n" + data["requirements_analysis"])
                if "architecture" in data:
                    sections.append("### ğŸ— ì•„í‚¤í…ì²˜ ì œì•ˆ\n" + data["architecture"])
                if "experiments" in data:
                    sections.append("### ğŸ§ª ì‹¤í—˜ í”Œëœ\n" + data["experiments"])

            # 2. ë¡œë“œë§µì—ì„œ ì°¸ê³  ëª¨ë¸ / ì•„í‚¤í…ì²˜ ê°€ì´ë“œ / ê·¼ê±° / ë¯¸ë˜ ë°©í–¥ë„ ë³´ì—¬ì£¼ê¸°
            ref_models = roadmap.get("reference_models") or []
            if ref_models:
                sections.append("### ğŸ§© ì°¸ê³  ëª¨ë¸\n" + "\n".join(f"- {m}" for m in ref_models))

            if roadmap.get("architecture_guidance"):
                sections.append("### ğŸ› ì•„í‚¤í…ì²˜ ê°€ì´ë“œ\n" + roadmap["architecture_guidance"])

            if roadmap.get("justification"):
                sections.append("### ğŸ“Œ ì„¤ê³„ ê·¼ê±°\n" + roadmap["justification"])

            future_dirs = roadmap.get("future_directions") or []
            if future_dirs:
                sections.append("### ğŸ”­ Future Work ì œì•ˆ\n" + "\n".join(f"- {d}" for d in future_dirs))

            # ìµœì¢… ì¶œë ¥ ì¡°í•©
            if sections:
                answer_text = "\n\n".join(sections)
            else:
                # í˜¹ì‹œ ìœ„ì—ì„œ ë‹¤ ì‹¤íŒ¨í•˜ë©´ ê·¸ëƒ¥ ì „ì²´ payloadë¥¼ JSONìœ¼ë¡œ ë³´ì—¬ì£¼ê¸°
                answer_text = "```json\n" + json.dumps(payload, ensure_ascii=False, indent=2) + "\n```"

            st.markdown(answer_text)

            if show_raw:
                with st.expander("ë””ë²„ê·¸: raw stdout"):
                    st.code("\n".join(raw_logs), language="bash")

        # 3) assistant ë©”ì‹œì§€ë¥¼ ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        st.session_state.messages.append({"role": "assistant", "content": answer_text})

    except Exception as e:
        error_msg = f"ì—ëŸ¬ ë°œìƒ: {e}"
        with st.chat_message("assistant"):
            st.error(error_msg)
        st.session_state.messages.append({"role": "assistant", "content": error_msg})
